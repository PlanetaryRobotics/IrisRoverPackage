import os
import errno
import gheconnector
import reportconfig
import planvalidator
import datetime
import csv
import numpy as np


class ProjectMetrics:
    """ Object class to gather and store project metrics from the following sources: GitHub issues, compiled SLOC
    report, compiled component structure report. The compiled reports are of a format specific to the F Prime framework
    since they are generated by the make process rather than a particular tool.

    """
    # issue dict accessors
    TD = "Timeline Delta"
    TT = "Timeline Total"
    OV = "Overall"
    UL = "Unlabelled"
    NEW = "Created"
    DONE = "Closed"
    OPEN = "Open"
    DATE = "Date"

    # SLOC file naming conventions, directories, dict accessors, and list constants
    # # Individual sources
    SLOC_AC_FILE = "ac_sloc.txt"
    SLOC_MC_FILE = "written_sloc.txt"
    SLOC_XML_FILE = "xml_sloc.txt"
    AC = "Automatic "
    MC = "Manual "
    XML = "XML "

    # # Compiled sources
    SLOC_REP = "SlocReport.csv"
    SMOD = "Module"  # delineates the header for the component csv section of the sloc report file
    STOT = "TOTAL"  # a column in the sloc report file we want to ignore

    # COMPonent file naming conventions and dict accessors
    COMP_REP = "ComponentReport.txt"
    CSTATS = "Stats:"  # delineates the start of the individual component comma seperated values
    CCOMP = "Component"  # delineates the header for the component csv section of the comp report file

    COMP_FILE = "ComponentReport.txt"
    COMP_ROLLUPS = ["Ports"]
    TP = "Total Ports"

    EV = planvalidator.PlanValidator.EV
    XV = "Expected Value"
    CV = "Current Value"

    def __init__(self, config_file=None, config_opts=None, git_api_key=None, zen_api_key=None):
        """ Object initializer.

        :param config_file: path to file with config information
        :param config_opts: Configuration options bundle, instance of reportconfig.ReportConfig. Will be overwritten by
            a new object if config_file is specified.
        :param git_api_key: GitHub api key to use with configuration bundle builder. Will overwrite existing keys.
        :param zen_api_key: ZenHub api key to use with configuration bundle builder. Will overwrite existing keys.
        """
        # initialize the various member lists and dicts
        if config_opts is None and config_file is None:
            return
        elif config_opts is None:
            config_opts = reportconfig.ReportConfiguration(config_file)
        if git_api_key:
            config_opts.git_api_key = git_api_key
        if zen_api_key:
            config_opts.zen_api_key = zen_api_key
        self._ghe_conn = gheconnector.GitHubConnector(config_options=config_opts)
        self.pv = planvalidator.PlanValidator(config_opts.metrics_ev_plan_files)

        self.metrics = self

        self._issue_delta_timelines = {self.OV: {self.NEW: {}, self.DONE: {}}}

        self.issue_deltas = {}
        self.issue_totals = {}
        self.issue_dates = []

        self.dir_comp_map = {}

        self.sloc_data = {}
        self.sloc_hist = []
        self.sloc_totals = {}

        self.comp_data = {}
        self.comp_hist = []
        self.comp_totals = {}

        self.plan_task_list = []
        self.plan_dict = {}

        self.task_gates = {}
        self.plan_totals = {}
        self.plan_progress = {}

        self.task_items_header = [self.pv.TASK,
                                  self.pv.ENG,
                                  self.pv.REL,
                                  self.pv.START,
                                  self.pv.END,
                                  self.pv.EV,
                                  self.XV,
                                  self.CV
                                  ]

        self.expand_make_target_modules(config_opts)
        self.generate_component_data(config_opts)
        self.generate_plan_data(config_opts)
        self.generate_issue_data(config_opts)

    def generate_plan_data(self, config_opts):
        """Generates plan data from the plans ingested in the self._pv object. Will also generate task and plan data
        metrics for the first plan in the list.

        :param config_opts: Configuration options bundle, instance of reportconfig.ReportConfig.
        :return:
        """
        for plan_index, plan_file in enumerate(config_opts.metrics_ev_plan_files):
            # if defined local file, get history from file
            plan_key = self.pv.generate_plan_file_key(plan_file)
            self.plan_totals[plan_key] = self.generate_plan_metrics(self.pv.plans[plan_key][self.pv.TASK], plan_index == 0)
            if plan_index == 0:
                self.plan_dict = self.pv.plans[plan_key][self.pv.TASK]
                self.plan_task_list = self.pv.plans[plan_key][self.pv.TASK_LIST]
                self.task_gates = {task:
                                   {gate: self.plan_dict[task][gate][self.pv.EV]
                                    for gate in self.plan_dict[task][self.pv.GATE_LIST]}
                                   for task in self.plan_task_list}
                pass

    def generate_plan_metrics(self, plan_dict, primary):
        """

        :param plan_dict:
        :param primary:
        :return:
        """
        plan_deltas = {}
        today = datetime.date.today()
        for task in plan_dict:
            if primary:
                plan_dict[task][self.XV] = 0.
                plan_dict[task][self.CV] = 0.
            for gate in plan_dict[task][self.pv.GATE_LIST]:
                date = plan_dict[task][gate][self.pv.END]
                contribution = plan_dict[task][gate][self.pv.EV]
                if date not in plan_deltas:
                    plan_deltas[date] = contribution
                else:
                    plan_deltas[date] += contribution
                if date + datetime.timedelta(-1) not in plan_deltas:
                    plan_deltas[date + datetime.timedelta(-1)] = 0.
                if primary and today >= date:
                    plan_dict[task][self.XV] += contribution
        plan_trend = {self.DATE: sorted(plan_deltas.keys())}
        plan_trend[self.pv.EV] = [plan_deltas[day] for day in plan_trend[self.DATE]]

        for index, date in enumerate(plan_deltas):
            if index == 0:
                continue
            plan_trend[self.EV][index] += plan_trend[self.EV][index - 1]
        return plan_trend

    def _retrieve_git_directory(self, repo_name, target_dir, ref='master', traverse_submodules=False):
        """ Retrieves a json representation of the indicated directory from the indicated git repo, from an optionally
        not master ref

        :param target_dir: directory to search in the target repository
        :param repo_name: name of the github repository to search. expects owning org as well. ex: "organization/repository"
        :param ref: optional. defaults to 'master'.
        :return: GitHub response as list of content json blobs
        """
        return self._ghe_conn.get_contents(repo_name, path=target_dir, ref=ref, traverse_submodules=traverse_submodules)

    def _retrieve_git_file(self, repo_name, target_file, ref='master', target_dir=None, alt_target_pattern=None,
                           traverse_submodules=False):
        """Retrieves a file's raw content from the indicated git repo, from an optionally not master ref. If the
        optional alt_target_pattern is specified, will use a file that matches the pattern if the primary specified
        is not found. If multiple files match the pattern, the last found by traversing the directory list will
        be used.

        :param repo_name:
        :param ref:
        :param target_file:
        :param target_dir:
        :param alt_target_pattern:
        :return: List of file lines
        """

        # sort input into expected formats
        if target_dir is None:
            if target_file.rfind('/') <= -1:
                target_dir = ''
            else:
                target_dir = target_file[:target_file.rfind('/')]
        # if target_dir and not target_dir.endswith('/'):
        #     target_dir = target_dir + '/'
        if not target_file.startswith(target_dir):
            target_file = target_dir + '/' + target_file

        # make sure the directory exists, find an appropriate file target_path for retrieval
        contents = self._retrieve_git_directory(repo_name, target_dir, ref, traverse_submodules)
        target_path = ""
        for item in contents:
            current_path = item.get("path")
            if target_file in current_path:
                target_path = current_path
                break
            elif alt_target_pattern and alt_target_pattern in item.get("path"):
                target_path = current_path

        if not target_path:
            raise IOError("Unsuccessful retrieval of file '{}/{}' on branch {}. Error code: {}. Content: {}"
                          .format(repo_name, target_file, ref, 404, ""))

        # if we're getting a different file that matches the alt_patter, tell the user.
        if target_file not in target_path:
            print("INFO: Found alternate file for '{}' at '{}'".format(target_file, target_path))

        # grab the file we've decided on
        body = self._ghe_conn.get_raw_file(repo_name, path=target_path, ref=ref, traverse_submodules=traverse_submodules)
        return body.replace("\r", "").split("\n")

    @staticmethod
    def generate_make_target_dict(lines):
        """ Generates a target dictionary from a file string.

        :param lines: Lines from a returned file.
        :return:
        """
        target_dict = {}
        entries = []
        append = False
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            if append:
                entries[-1] = entries[-1] + line
                if not line.endswith('\\'):
                    append = False
            else:
                entries.append(line)
                if line.endswith('\\'):
                    append = True
        for entry in entries:
            entry = entry.strip()
            if not entry:
                continue
            entry = entry.split(':=')
            deployment, items = entry[0].strip(), entry[1].strip()
            if deployment in target_dict:
                target_dict[deployment].update(items.replace('\\', ' ').split())
            else:
                target_dict[deployment] = set(items.replace('\\', ' ').split())
        return target_dict

    def generate_platform_component_list_recursive(self, make_target, make_target_dict, component_dict=None, stack=None):
        """

        :param make_target:
        :param make_target_dict:
        :param component_dict: current dictionary
        :param stack:
        :return:
        """
        if component_dict is None:
            component_dict = {}

        if make_target_dict.get("DEPLOYMENTS") and make_target in make_target_dict['DEPLOYMENTS']:
            make_target = make_target + "_MODULES"

        # no guarantee of non-circular make modules, so we check for circular calls and just refuse to do anything with them
        if stack is None:
            stack = {make_target}
        elif make_target in stack:
            return

        stack.add(make_target)
        for component in make_target_dict[make_target]:
            if component.startswith('$('):
                component_dict.update(self.generate_platform_component_list_recursive(component[2:-1], make_target_dict,
                                                                                      component_dict, stack))
            else:
                component_dict[component] = [component]
        stack.remove(make_target)
        return component_dict

    def expand_make_target_modules(self, config_opts):
        """

        :param config_opts:
        :return:
        """
        if config_opts.metrics_make_deployment is None:
            # nothing to do
            return

        lines = ""
        for repo_name in config_opts.git_repo_list:
            try:
                lines = self._retrieve_git_file(repo_name, config_opts.metrics_make_location, config_opts.git_branch,
                                                traverse_submodules=True)
                # if successful, assume other repos aren't needed and break
                print("INFO: Retrieved {}/{} on {}"
                      .format(repo_name, config_opts.metrics_make_location, config_opts.git_branch))
                break
            except IOError as err:
                print("WARN: " + err.message)
                continue
        if not lines:
            # nothing to do if found nothing
            return

        # parse the lines from the discovered file into a dictionary of (potentially) recursive calls, then pull
        # them out starting with the make target and resolving nested calls recursively. store in expected place for
        # further processing in later functions
        target_dict = self.generate_make_target_dict(lines)
        platform_comp_list = self.generate_platform_component_list_recursive(config_opts.metrics_make_deployment, target_dict)
        for component in platform_comp_list:
            if component in config_opts.dir_comp_map:
                # if component is already in the dir_comp_map, it's mapped and we don't really care what to
                continue
            else:
                config_opts.dir_comp_map[component] = component
                config_opts.components.add(component)
        return

    def generate_issue_data(self, config_opts):
        """

        :param config_opts:
        :return:
        """
        progress_deltas = {}
        # if we didn't just import data, iterate through issues to get data
        if self._issue_delta_timelines == {self.OV: {self.NEW: {}, self.DONE: {}}}:
            for repo_name in config_opts.git_repo_list:
                try:
                    issues = self._ghe_conn.get_issues(repo_name)
                except BaseException as err:
                    print("WARN: {}".format(err.message))
                for issue_number in list(issues.keys()):
                    # if issue_number % 100 == 0 or issue_number == 682:
                    #     pass
                    issue = issues[issue_number]

                    if not self.validate_issue_inclusion(config_opts, issue):
                        continue

                    # for every issue, add one to the appropriate created date count
                    created_date = issue.get("created_at")
                    if created_date is not None:
                        created_date = datetime.datetime.strptime(created_date[:10], "%Y-%m-%d").date()
                        if self._issue_delta_timelines[self.OV][self.NEW].get(created_date) is None:
                            self._issue_delta_timelines[self.OV][self.NEW][created_date] = 1
                        else:
                            self._issue_delta_timelines[self.OV][self.NEW][created_date] += 1

                    task, gate, earned_value = self.pv.find_issue_in_plan(issue)

                    event_labels, issue_open = self.process_issue_events(config_opts, issue, progress_deltas,
                                                                         task, earned_value)

                    # check to see if it was closed without a close event for some reason
                    closed_date = issue.get("closed_at")
                    if closed_date is not None and issue_open:
                        print("INFO: Issue {} is closed, but no closed event was processed. Close applied outside "
                              "of event processing loop.".format(issue_number))
                        closed_date = datetime.datetime.strptime(closed_date[:10], "%Y-%m-%d").date()
                        self.process_issue_close_event(None, event_labels, progress_deltas, earned_value, "closed",
                                                       closed_date)

                    # make sure that we don't have mismatched labels between events and current issue
                    self.validate_issue_label_state(issue, event_labels, list(self.task_gates.keys()))

        # pre-process the issue delta dict into sequential date lists for each label
        for label in list(self._issue_delta_timelines.keys()):
            self.issue_deltas[label] = {self.NEW: [], self.DONE: [], self.OPEN: []}
            self.issue_totals[label] = {self.NEW: [], self.DONE: [], self.OPEN: []}

        # oldest applicable date is the oldest creation date, by definition
        if not self._issue_delta_timelines[self.OV][self.NEW]:
            return
        cur_date = sorted(self._issue_delta_timelines[self.OV][self.NEW].keys())[0]
        today = datetime.date.today()
        # build lists of dates, deltas, and totals
        while cur_date <= today:
            self.issue_dates.append(cur_date)
            for label in list(self._issue_delta_timelines.keys()):
                if len(self.issue_dates) > 1:
                    prev_new = self.issue_totals[label][self.NEW][-1]
                    prev_done = self.issue_totals[label][self.DONE][-1]
                    prev_open = self.issue_totals[label][self.OPEN][-1]
                else:
                    prev_new = 0
                    prev_done = 0
                    prev_open = 0

                created = self._issue_delta_timelines[label][self.NEW].get(cur_date)
                if created is None:
                    created = 0
                self.issue_deltas[label][self.NEW].append(created)
                self.issue_totals[label][self.NEW].append(created + prev_new)

                closed = self._issue_delta_timelines[label][self.DONE].get(cur_date)
                if closed is None:
                    closed = 0
                self.issue_deltas[label][self.DONE].append(closed)
                self.issue_totals[label][self.DONE].append(closed + prev_done)

                opened = created - closed
                self.issue_deltas[label][self.OPEN].append(opened)
                self.issue_totals[label][self.OPEN].append(opened + prev_open)

            cur_date += datetime.timedelta(days=1)

        # process the plan deltas if we have any
        if progress_deltas:
            start = sorted(progress_deltas.keys())[0]
            date_list = [start + datetime.timedelta(x + 1) for x in range((today - start).days)]
            self.plan_progress[self.EV] = [progress_deltas[start]]
            self.plan_progress[self.DATE] = [start]
            for day in date_list:
                self.plan_progress[self.DATE].append(day)
                self.plan_progress[self.EV].append(self.plan_progress[self.EV][-1])
                if day in list(progress_deltas.keys()):
                    self.plan_progress[self.EV][-1] += progress_deltas[day]
        # make sure progress dates match plan dates
        return

    def validate_issue_inclusion(self, config_opts, issue_json):
        """

        :param config_opts:
        :param issue_json:
        :return:
        """
        # filter issues by milestone
        milestone = issue_json.get("milestone")
        if milestone is not None:
            milestone = str(milestone.get("title"))
        if milestone in config_opts.excluded_milestones:
            return False
        if config_opts.included_milestones != ['*'] and milestone not in config_opts.included_milestones:
            return False

        # intentionally skip issues whose label is in the excluded label configuration list
        # note that this is an aggressive removal, and it will skip issues with this label even if it also
        # has an explicit include label
        for label_json in issue_json.get("labels"):
            label_name = str(label_json.get("name"))
            if label_name in config_opts.git_label_mappings:
                label_name = config_opts.git_label_mappings[label_name]
            if label_name in config_opts.excluded_labels:
                return False
        return True

    def validate_issue_label_state(self, issue_json, event_labels, task_labels):
        """

        :param issue_json:
        :param event_labels:
        :param task_labels:
        :return:
        """
        issue_labels = set()
        issue_name = issue_json.get("title")
        for label_json in issue_json.get("labels"):
            label_name = str(label_json.get("name"))
            issue_labels.add(label_name)
        for label in event_labels:
            if label not in issue_labels:
                print("WARN: Label \"{}\" was found in labeling events, but is not not on issue \"{}\" currently. It "
                      "was likely changed to a new name and not indicated in the config. This may cause graphs to "
                      "appear invalid.".format(label, issue_name))
        for label in issue_labels:
            if label not in event_labels and label not in task_labels:
                print("WARN: Label \"{}\" is on issue \"{}\" currently, but was not found in labeling events. This is "
                      "likely a renamed label that was not indicated in the config. This may cause graphs to appear "
                      "invalid.".format(label, issue_name))

    def process_issue_events(self, config_opts, issue_json, progress_deltas, task, earned_value):
        """

        :param config_opts:
        :param issue_json:
        :param progress_deltas:
        :param task:
        :param earned_value:
        :return:
        """
        # set key trackers of issue
        event_labels = set()
        issue_open = True

        # for every issue, check it's events for label changes
        for event_json in issue_json.get("events"):
            if event_json is None:
                continue
            event_type = event_json.get("event")
            if event_type == "labeled":
                event_labels = self.process_issue_label_event(config_opts, event_json, issue_open, event_labels,
                                                              progress_deltas, task, earned_value)
            elif event_type == "unlabeled":
                event_labels = self.process_issue_label_event(config_opts, event_json, issue_open, event_labels,
                                                              progress_deltas, task, earned_value)
            elif event_type == "closed":
                issue_open = self.process_issue_close_event(event_json, event_labels, progress_deltas,
                                                            task, earned_value)
            elif event_type == "reopened":
                issue_open = self.process_issue_close_event(event_json, event_labels, progress_deltas,
                                                            task, earned_value)
            elif event_type == "milestoned":
                pass
            elif event_type == "demilestoned":
                pass
        return event_labels, issue_open

    def process_issue_label_event(self, config_opts, event_json, issue_open, event_labels, progress_deltas, task, earned_value):
        """

        :param config_opts:
        :param event_json:
        :param issue_open:
        :param event_labels:
        :param progress_deltas:
        :param task:
        :param earned_value:
        :return:
        """
        label_json = event_json.get("label")
        label_name = str(label_json.get("name"))
        if label_name in config_opts.git_label_mappings:
            label_name = config_opts.git_label_mappings[label_name]
        if label_name in config_opts.excluded_labels:
            return event_labels
        if label_name in list(self.task_gates.keys()):
            return event_labels

        event_type = event_json.get("event")
        event_date = datetime.datetime.strptime(event_json.get("created_at")[:10], "%Y-%m-%d").date()
        if event_type == "labeled":
            modifier = 1
            event_labels.add(label_name)
        elif event_type == "unlabeled":
            modifier = -1
            try:
                event_labels.remove(label_name)
            except KeyError as err:
                event_labels.discard(label_name)
                print("WARN: {} removed from labels before being added. It was likely renamed after it was added, and "
                      "not indicated in the config. This may cause graphs to appear invalid.".format(label_name))
        else:
            return event_labels

        if label_name.endswith('%') and label_name[:-1].isdigit() and issue_open and earned_value > 0:
            # if the issue is open, adjust these as described. if the issue is closed, this will already have been
            # removed so we can safely skip.
            percent = float(label_name[:-1]) / 100.
            if progress_deltas.get(event_date) is None:
                progress_deltas[event_date] = earned_value * percent * modifier
            else:
                progress_deltas[event_date] += earned_value * percent * modifier
            self.plan_dict[task][self.CV] += earned_value * percent * modifier
            # don't do additional processing on % labels
            return event_labels

        if self._issue_delta_timelines.get(label_name) is None:
            self._issue_delta_timelines[label_name] = {self.NEW: {}, self.DONE: {}}
        if self._issue_delta_timelines[label_name][self.NEW].get(event_date) is None:
            self._issue_delta_timelines[label_name][self.NEW][event_date] = modifier
        else:
            self._issue_delta_timelines[label_name][self.NEW][event_date] += modifier
        if not issue_open:
            if self._issue_delta_timelines[label_name][self.DONE].get(event_date) is None:
                self._issue_delta_timelines[label_name][self.DONE][event_date] = modifier
            else:
                self._issue_delta_timelines[label_name][self.DONE][event_date] += modifier
        return event_labels

    def process_issue_close_event(self, event_json, event_labels, progress_deltas, task, earned_value, event_type=None, event_date=None):
        """

        :param event_json:
        :param event_labels:
        :param progress_deltas:
        :param task:
        :param earned_value:
        :param event_type:
        :param event_date:
        :return:
        """
        if event_json is not None:
            event_type = event_json.get("event")
            event_date = datetime.datetime.strptime(event_json.get("created_at")[:10], "%Y-%m-%d").date()
        if event_type == "closed":
            modifier = 1
            issue_open = False
        elif event_type == "reopened":
            modifier = -1
            issue_open = True
        else:
            return

        contribution = earned_value * modifier

        # basic overall issue closing, add to closed lists and add to progress deltas
        if self._issue_delta_timelines[self.OV][self.DONE].get(event_date) is None:
            self._issue_delta_timelines[self.OV][self.DONE][event_date] = modifier
        else:
            self._issue_delta_timelines[self.OV][self.DONE][event_date] += modifier
        if earned_value > 0:
            if progress_deltas.get(event_date) is None:
                progress_deltas[event_date] = contribution
            else:
                progress_deltas[event_date] += contribution
            self.plan_dict[task][self.CV] += contribution

        # for every issue, add one to the appropriate closed date count for its current labels as determined by event
        # processing. don't need to process these for renames or invalid names, since they are added to the set after
        # checking already
        for label_name in event_labels:

            # if we find a percentile label, add or remove the appropriate EV amounts
            if label_name.endswith('%') and earned_value > 0:
                percent = float(label_name[:-1]) / 100.
                if progress_deltas.get(event_date) is None:
                    progress_deltas[event_date] = -contribution * percent
                else:
                    progress_deltas[event_date] -= contribution * percent
                self.plan_dict[task][self.CV] -= contribution * percent
                continue

            # add label dict if one is not present, complain because this should have happened
            if self._issue_delta_timelines.get(label_name) is None:
                self._issue_delta_timelines[label_name] = {self.NEW: {}, self.DONE: {}}
                print("WARN: Label '{}' was on a closed issue before being added. It was likely renamed after it was "
                      "added, and not indicated in the config. This may cause graphs to appear invalid."
                      .format(label_name))
            if self._issue_delta_timelines[label_name][self.DONE].get(event_date) is None:
                self._issue_delta_timelines[label_name][self.DONE][event_date] = modifier
            else:
                self._issue_delta_timelines[label_name][self.DONE][event_date] += modifier
        return issue_open

    def generate_component_data(self, config_opts):
        """

        :param config_opts:
        :return:
        """
        metrics_report_location = config_opts.metrics_component_report_location
        if metrics_report_location is None:
            metrics_report_location = config_opts.metrics_make_deployment
        elif metrics_report_location == '.':
            metrics_report_location = ""
        if metrics_report_location and not metrics_report_location.endswith('/'):
            metrics_report_location = metrics_report_location + '/'
        if metrics_report_location is not None:
            for repo_name in config_opts.git_repo_list:
                if not self.sloc_data:
                    sloc_file = metrics_report_location + config_opts.metrics_build_prefix + self.SLOC_REP
                    try:
                        lines = self._retrieve_git_file(repo_name, sloc_file, config_opts.git_branch, traverse_submodules=True)
                        if lines:
                            print("INFO: Retrieved {}/{} on {}"
                                  .format(repo_name, sloc_file, config_opts.git_branch))
                            self.sloc_data = self.process_compiled_sloc(config_opts, lines)
                    except IOError as err:
                        print("WARN: " + err.message)
                if not self.comp_data:
                    comp_file = metrics_report_location + self.COMP_REP
                    try:
                        lines = self._retrieve_git_file(repo_name, comp_file, config_opts.git_branch, traverse_submodules=True)
                        if lines:
                            print("INFO: Retrieved {}/{} on {}"
                                  .format(repo_name, comp_file, config_opts.git_branch))
                            self.comp_data = self.process_compiled_comp(config_opts, lines)
                    except IOError as err:
                        print("WARN: " + err.message)

        if not self.comp_data:
            self.comp_data = self.generate_comp_data(config_opts)
        if not self.sloc_data:
            self.sloc_data = self.generate_sloc_data(config_opts)
        self.sloc_hist, self.sloc_totals = \
            self.process_component_totals(self.sloc_data, config_opts.metrics_sloc_history, config_opts)
        self.comp_hist, self.comp_totals = \
            self.process_component_totals(self.comp_data, config_opts.metrics_comp_history, config_opts)
        return

    def process_compiled_sloc(self, config_opts, lines):
        """

        :param config_opts:
        :param lines:
        :return:
        """
        sloc_data = {}
        module_index = -1
        for line in lines:
            if not line or line.startswith("make[") or line.startswith("/usr/bin/gcc"):
                continue
            if line.startswith(self.SMOD):
                headers = line.split(',')
                for index, header in enumerate(headers):
                    headers[index] = header.strip()
                module_index = headers.index(self.SMOD)
                continue
            if module_index < 0:
                raise ValueError("Unexpected line in compiled sloc file. Aborting processing of unknown data "
                                 "structure. Line follows:\n\t{}".format(line))
            columns = line.split(',')
            module = columns[module_index].strip()
            if module not in config_opts.dir_comp_map:
                print("INFO: {} found in SLOC report, but is not among known components for this target."
                      .format(module))
                continue
            component = config_opts.dir_comp_map[module]
            if component not in sloc_data:
                sloc_data[component] = {}
            for index, value in enumerate(columns):
                if headers[index] == self.SMOD or headers[index] == self.STOT:
                    continue
                if headers[index] not in sloc_data[component]:
                    sloc_data[component][headers[index]] = int(value.strip())
                else:
                    sloc_data[component][headers[index]] += int(value.strip())
        for component in config_opts.components:
            if component in sloc_data:
                continue
            print("WARN: No SLOC found for component \"{}\" in compiled SlocReport file".format(component))
            sloc_data[component] = {}
            for header in headers:
                if header == self.SMOD or header == self.STOT:
                    continue
                sloc_data[component][header] = 0
        return sloc_data

    def process_compiled_comp(self, config_opts, lines):
        """

        :param config_opts:
        :param lines:
        :return:
        """
        comp_data = {}
        module_index = -1
        found_stats = False
        for line in lines:
            if not line:
                continue
            if line.startswith(self.CSTATS):
                found_stats = True
                continue
            if not found_stats:
                continue
            if line.startswith(self.CCOMP):
                headers = line.split(',')
                for index, header in enumerate(headers):
                    headers[index] = header.strip()
                module_index = headers.index(self.CCOMP)
                continue
            if module_index < 0:
                raise ValueError("Unexpected line in compiled comp report file. Aborting processing of unknown "
                                 "data structure. Line follows:\n\t{}".format(line))
            columns = line.split(',')
            module = columns[module_index][:columns[module_index].rfind('/')]
            if module.startswith('/'):
                module = module[1:]
            if module not in config_opts.dir_comp_map:
                print("INFO: {} found in ComponentReport, but is not among known components for this target."
                      .format(module))
                continue
            component = config_opts.dir_comp_map[module]
            if component not in comp_data:
                comp_data[component] = {}
            for index, value in enumerate(columns):
                if headers[index] == self.CCOMP:
                    continue
                if headers[index] not in comp_data[component]:
                    comp_data[component][headers[index]] = int(value.strip())
                else:
                    comp_data[component][headers[index]] += int(value.strip())
                # custom handling for total rollups. i.e. "Total Ports = Input Ports + Output Ports"
                for total in self.COMP_ROLLUPS:
                    if total in headers[index]:
                        total_key = "Total " + total
                        if total_key not in comp_data[component]:
                            comp_data[component][total_key] = int(value.strip())
                        else:
                            comp_data[component][total_key] += int(value.strip())
        return comp_data

    def generate_sloc_data(self, config_opts):
        """

        :param config_opts:
        :return:
        """
        for directory in list(config_opts.dir_comp_map.keys()):
            component = config_opts.dir_comp_map[directory]
            self.sloc_data[component] = {}
            for sloc_type in [(self.SLOC_MC_FILE, self.MC),
                              (self.SLOC_AC_FILE, self.AC),
                              (self.SLOC_XML_FILE, self.XML)]:
                sloc_file = config_opts.metrics_build_prefix + sloc_type[0]
                lines = None

                # go through listed repos until find the matching file
                for repo_name in config_opts.git_repo_list:
                    ref = config_opts.git_branch
                    target_dir = directory
                    try:
                        lines = self._retrieve_git_file(repo_name, sloc_file, ref, target_dir=target_dir,
                                                        traverse_submodules=True)
                    except IOError as err:
                        if " directory " in err.message:
                            print("WARN: {}".format(err.message))
                if lines is None:
                    continue
                else:
                    header = lines[0]
                    lines = lines[1:]
                    columns = header.split()
                    if "file" not in columns:
                        for column in columns:
                            if self.sloc_data[component].get(sloc_type[1] + column) is None:
                                self.sloc_data[component][sloc_type[1] + column] = 0
                    else:
                        for line in lines:
                            words = line.split()
                            if len(words) < len(columns):
                                continue
                            index = columns.index("file")
                            if words[index] == "total":
                                for i in range(index - 1):
                                    if self.sloc_data[component].get(sloc_type[1] + columns[i]) is None:
                                        self.sloc_data[component][sloc_type[1] + columns[i]] = 0
                                    self.sloc_data[component][sloc_type[1] + columns[i]] += int(words[i])
        return self.sloc_data

    def generate_comp_data(self, config_opts):
        """

        :param config_opts:
        :return:
        """
        for directory in list(config_opts.dir_comp_map.keys()):
            component = config_opts.dir_comp_map[directory]
            self.comp_data[component] = {}
            comp = component
            if '/' in comp:
                comp = comp[comp.rfind('/') + 1:]
            # comp_file = directory + "/" + config_opts.metrics_build_prefix + component + self.COMP_FILE_SUFFIX
            comp_file = comp + self.COMP_FILE
            lines = None

            # go through listed repos until find the matching file
            for repo_name in config_opts.git_repo_list:
                ref = config_opts.git_branch
                target_dir = directory
                try:
                    lines = self._retrieve_git_file(repo_name, comp_file, ref, target_dir=target_dir,
                                                    alt_target_pattern=self.COMP_FILE,
                                                    traverse_submodules=True)
                except IOError as err:
                    if " directory " in err.message:
                        print("WARN: {}".format(err.message))
            if lines is None:
                continue
            else:
                for line in lines:
                    if len(line) < 1 or line[0] == ' ' or ':' not in line:
                        continue
                    words = line.split(":")
                    if self.comp_data[component].get(words[0]) is None:
                        self.comp_data[component][words[0]] = int(words[1])
                    else:
                        self.comp_data[component][words[0]] += int(words[1])
                    # custom exception for total ports so we don't have to sum them manually later
                    if "Ports" in words[0]:
                        if self.comp_data[component].get(self.TP) is None:
                            self.comp_data[component][self.TP] = 0
                        self.comp_data[component][self.TP] += int(words[1])
        return self.comp_data

    def process_component_totals(self, component_data, history_file, config_opts):
        """

        :param component_data:
        :param history_file:
        :param config_opts:
        :return:
        """
        current_totals = {}
        component_totals = {}
        # generate totals for history file
        for component in list(component_data.keys()):
            for item in list(component_data[component].keys()):
                if item not in current_totals:
                    current_totals[item] = component_data[component][item]
                else:
                    current_totals[item] += component_data[component][item]

        today = datetime.date.today()

        component_hist = self.process_component_history(history_file, current_totals, config_opts)

        # parse individual dates into a full date range list
        temp = np.array(component_hist)
        temp = temp.T
        for row in temp:
            if row[0] == self.DATE:
                component_totals[row[0]] = np.array([datetime.datetime.strptime(i, "%Y-%m-%d").date() for i in row[1:]])
            else:
                component_totals[row[0]] = np.array([int(i) for i in row[1:]])
        index = 0
        next_date = component_totals[self.DATE][index] + datetime.timedelta(days=1)
        while next_date <= today:
            if str(next_date) != str(component_totals[self.DATE][index + 1]):
                for key in list(component_totals.keys()):
                    if key == self.DATE:
                        insert_val = next_date
                    else:
                        insert_val = component_totals[key][index]
                    component_totals[key] = np.insert(component_totals[key], index + 1, insert_val)
            next_date += datetime.timedelta(days=1)
            index += 1

        return component_hist, component_totals

    def process_component_history(self, history_file, totals, config_opts):
        """

        :param history_file:
        :param totals:
        :param config_opts:
        :return:
        """
        today = datetime.date.today()

        data = [[self.DATE] + sorted(totals.keys())]
        # if defined local file, get history from file
        if history_file is not None:
            try:
                with open(history_file, 'rb') as file_ptr:
                    comp_reader = csv.reader(file_ptr)
                    data = list(comp_reader)
            # if we can't open it, use default header data
            except IOError as err:
                pass

        # delete today's entry, if there is one already, so we don't get extra
        if data[-1][0] == str(today):
            del (data[-1])

        # rebuild history file with potentially new keys
        old_columns = data[0]

        header = list(totals.keys())
        for column in old_columns:
            if column not in header:
                header.append(column)
        if self.DATE in header:
            header.remove(self.DATE)
        header = [self.DATE] + sorted(header)
        rebuild = [header]
        for entry in range(1, len(data)):
            newrow = []
            for col in range(len(header)):
                if header[col] not in data[0]:
                    newrow.append(0)
                else:
                    newrow.append(data[entry][data[0].index(header[col])])
            rebuild.append(newrow)

        newrow = [str(today)]
        for col in range(1, len(header)):
            if totals.get(header[col]) is None:
                newrow.append('0')
            else:
                newrow.append(totals[header[col]])
        rebuild.append(newrow)

        # if we have a defined file and we're working locally, store an updated copy.
        # this will also create a new one with today's history if one didn't exist.
        if history_file is not None:
            path = history_file.split('/')
            directory = '.'
            for item in path:
                directory = directory + '/' + item
                if directory == './' + history_file:
                    break
                try:
                    os.makedirs(directory)
                except OSError as e:
                    if e.errno != errno.EEXIST:
                        raise
            try:
                with open(history_file, 'wb') as file_ptr:
                    writer = csv.writer(file_ptr)
                    writer.writerows(rebuild)
            except IOError as err:
                print("Unable to update " + history_file + ". Error message: " + err.message)
        return rebuild


if __name__ == '__main__':
    raise NotImplementedError()
